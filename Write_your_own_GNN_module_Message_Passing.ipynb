{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carlos1729/DGL/blob/main/Write_your_own_GNN_module_Message_Passing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hvir2oKaQ0U9"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU77Cl-ERGzQ",
        "outputId": "8302c64b-47ee-4675-d0a1-33d712f11fcf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/cu116/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/cu116/dgl-1.1.2%2Bcu116-cp310-cp310-manylinux1_x86_64.whl (92.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.3)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.7.22)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.2+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9mHY0l5RG2R",
        "outputId": "5af08378-cdd8-46b1-ee11-3e9b2669823a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Collecting dglgo\n",
            "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.9.0)\n",
            "Collecting isort>=5.10.1 (from dglgo)\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autopep8>=1.6.0 (from dglgo)\n",
            "  Downloading autopep8-2.0.4-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpydoc>=1.1.0 (from dglgo)\n",
            "  Downloading numpydoc-1.6.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.10.13)\n",
            "Collecting ruamel.yaml>=0.17.20 (from dglgo)\n",
            "  Downloading ruamel.yaml-0.18.3-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (6.0.1)\n",
            "Collecting ogb>=1.3.3 (from dglgo)\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit-pypi (from dglgo)\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.2.2)\n",
            "Collecting pycodestyle>=2.10.0 (from autopep8>=1.6.0->dglgo)\n",
            "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: sphinx>=5 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (5.0.2)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (4.66.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb>=1.3.3->dglgo)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (4.5.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.20->dglgo)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.2.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.0->dglgo) (8.1.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->dglgo) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb>=1.3.3->dglgo)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2023.3.post1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.6)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.16.1)\n",
            "Requirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (0.18.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.13.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb>=1.3.3->dglgo) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=d513aefead411cb59bf5f189b6bd86fcb00625108bb9f09d7c76c0b1b3adc772\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, ruamel.yaml.clib, rdkit-pypi, pycodestyle, isort, ruamel.yaml, outdated, autopep8, ogb, numpydoc, dglgo\n",
            "Successfully installed autopep8-2.0.4 dglgo-0.0.2 isort-5.12.0 littleutils-0.2.2 numpydoc-1.6.0 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.11.1 rdkit-pypi-2022.9.5 ruamel.yaml-0.18.3 ruamel.yaml.clib-0.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtjcxnXeQ0VA"
      },
      "source": [
        "\n",
        "Write your own GNN module\n",
        "=========================\n",
        "\n",
        "Sometimes, your model goes beyond simply stacking existing GNN modules.\n",
        "For example, you would like to invent a new way of aggregating neighbor\n",
        "information by considering node importance or edge weights.\n",
        "\n",
        "By the end of this tutorial you will be able to\n",
        "\n",
        "-  Understand DGL’s message passing APIs.\n",
        "-  Implement GraphSAGE convolution module by your own.\n",
        "\n",
        "This tutorial assumes that you already know :doc:`the basics of training a\n",
        "GNN for node classification <1_introduction>`.\n",
        "\n",
        "(Time estimate: 10 minutes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFH9YeQAQ0VD",
        "outputId": "b1b017e2-7750-4a88-bbc8-4fa6b905d624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaYBKt0vQ0VD"
      },
      "source": [
        "Message passing and GNNs\n",
        "------------------------\n",
        "\n",
        "DGL follows the *message passing paradigm* inspired by the Message\n",
        "Passing Neural Network proposed by `Gilmer et\n",
        "al. <https://arxiv.org/abs/1704.01212>`__ Essentially, they found many\n",
        "GNN models can fit into the following framework:\n",
        "\n",
        "\\begin{align}m_{u\\to v}^{(l)} = M^{(l)}\\left(h_v^{(l-1)}, h_u^{(l-1)}, e_{u\\to v}^{(l-1)}\\right)\\end{align}\n",
        "\n",
        "\\begin{align}m_{v}^{(l)} = \\sum_{u\\in\\mathcal{N}(v)}m_{u\\to v}^{(l)}\\end{align}\n",
        "\n",
        "\\begin{align}h_v^{(l)} = U^{(l)}\\left(h_v^{(l-1)}, m_v^{(l)}\\right)\\end{align}\n",
        "\n",
        "where DGL calls $M^{(l)}$ the *message function*, $\\sum$ the\n",
        "*reduce function* and $U^{(l)}$ the *update function*. Note that\n",
        "$\\sum$ here can represent any function and is not necessarily a\n",
        "summation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF_B7j_QQ0VE"
      },
      "source": [
        "For example, the `GraphSAGE convolution (Hamilton et al.,\n",
        "2017) <https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf>`__\n",
        "takes the following mathematical form:\n",
        "\n",
        "\\begin{align}h_{\\mathcal{N}(v)}^k\\leftarrow \\text{Average}\\{h_u^{k-1},\\forall u\\in\\mathcal{N}(v)\\}\\end{align}\n",
        "\n",
        "\\begin{align}h_v^k\\leftarrow \\text{ReLU}\\left(W^k\\cdot \\text{CONCAT}(h_v^{k-1}, h_{\\mathcal{N}(v)}^k) \\right)\\end{align}\n",
        "\n",
        "You can see that message passing is directional: the message sent from\n",
        "one node $u$ to other node $v$ is not necessarily the same\n",
        "as the other message sent from node $v$ to node $u$ in the\n",
        "opposite direction.\n",
        "\n",
        "Although DGL has builtin support of GraphSAGE via\n",
        ":class:`dgl.nn.SAGEConv <dgl.nn.pytorch.SAGEConv>`,\n",
        "here is how you can implement GraphSAGE convolution in DGL by your own.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zzfjk5jiQ0VE"
      },
      "outputs": [],
      "source": [
        "import dgl.function as fn\n",
        "\n",
        "# This line imports functions from the dgl.function module. In this code, functions like copy_u and mean from\n",
        "# DGL's function library are used for message passing.\n",
        "\n",
        "class SAGEConv(nn.Module):\n",
        "    \"\"\"Graph convolution module used by the GraphSAGE model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_feat : int\n",
        "        Input feature size.\n",
        "    out_feat : int\n",
        "        Output feature size.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_feat, out_feat):#This line defines the constructor for the SAGEConv class, which takes two arguments:\n",
        "                                          # in_feat: The size of the input features for each node.\n",
        "                                          # out_feat: The size of the output features that this convolution will produc\n",
        "\n",
        "        super(SAGEConv, self).__init__() #Calls the constructor of the parent class nn.Module, which is necessary when creating a custom\n",
        "        # neural network module.A linear submodule for projecting the input and neighbor feature to the output.\n",
        "\n",
        "        self.linear = nn.Linear(in_feat * 2, out_feat)#Initializes a linear transformation (nn.Linear) that combines input and neighbor features\n",
        "        # to produce output features. The input dimension of the linear layer is in_feat * 2 because it concatenates the input feature (h) and\n",
        "        #  the aggregated neighbor feature (h_N)\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        \"\"\"Forward computation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        g : Graph\n",
        "            The input graph.\n",
        "        h : Tensor\n",
        "            The input node feature.\n",
        "        \"\"\"\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            # Assigns the input node features h to the nodes of the graph g with the key 'h'. This is a common practice in DGL to set initial node features.\n",
        "            # update_all is a message passing API.\n",
        "            g.update_all(message_func=fn.copy_u('h', 'm'), reduce_func=fn.mean('m', 'h_N'))\n",
        "            # Utilizes the update_all method to perform message passing in the graph. Specifically:\n",
        "            # message_func (fn.copy_u('h', 'm')): Defines the message function, which copies the node feature 'h' to the message 'm'. This function is used to gather information from neighboring nodes.\n",
        "            # reduce_func (fn.mean('m', 'h_N')): Specifies the reduce function, which calculates the mean of the messages 'm' received from neighbors and stores the result in the node feature 'h_N'.\n",
        "            h_N = g.ndata['h_N']\n",
        "            # Retrieves the node features 'h_N' from the graph, which now contain the aggregated information from neighboring nodes.\n",
        "            h_total = torch.cat([h, h_N], dim=1)\n",
        "            return self.linear(h_total)\n",
        "            #Concatenates the input node features h and the aggregated neighbor features h_N along the specified dimension (dim=1) to form the\n",
        "            # total input for the linear layer return self.linear(h_total):\n",
        "\n",
        "            # Passes the concatenated features h_total through a linear transformation defined earlier (self.linear).\n",
        "            # The result is the output of the GraphSAGE convolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dBtws-XQ0VF"
      },
      "source": [
        "The central piece in this code is the\n",
        ":func:`g.update_all <dgl.DGLGraph.update_all>`\n",
        "function, which gathers and averages the neighbor features. There are\n",
        "three concepts here:\n",
        "\n",
        "* Message function ``fn.copy_u('h', 'm')`` that\n",
        "  copies the node feature under name ``'h'`` as *messages* sent to\n",
        "  neighbors.\n",
        "\n",
        "* Reduce function ``fn.mean('m', 'h_N')`` that averages\n",
        "  all the received messages under name ``'m'`` and saves the result as a\n",
        "  new node feature ``'h_N'``.\n",
        "\n",
        "* ``update_all`` tells DGL to trigger the\n",
        "  message and reduce functions for all the nodes and edges.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR_18E6MQ0VF"
      },
      "source": [
        "Afterwards, you can stack your own GraphSAGE convolution layers to form\n",
        "a multi-layer GraphSAGE network.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qbAGp3feQ0VF"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats)\n",
        "        self.conv2 = SAGEConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VHSE4WLQ0VF"
      },
      "source": [
        "Training loop\n",
        "~~~~~~~~~~~~~\n",
        "The following code for data loading and training loop is directly copied\n",
        "from the introduction tutorial.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADyHnNLGQ0VG",
        "outputId": "e49b8fa8-a4c6-4db5-e007-964618ab505a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "Extracting file to /root/.dgl/cora_v2_d697a464\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "In epoch 0, loss: 1.951, val acc: 0.114 (best 0.114), test acc: 0.103 (best 0.103)\n",
            "In epoch 5, loss: 1.880, val acc: 0.308 (best 0.308), test acc: 0.289 (best 0.289)\n",
            "In epoch 10, loss: 1.741, val acc: 0.414 (best 0.414), test acc: 0.397 (best 0.397)\n",
            "In epoch 15, loss: 1.527, val acc: 0.586 (best 0.586), test acc: 0.602 (best 0.602)\n",
            "In epoch 20, loss: 1.246, val acc: 0.640 (best 0.640), test acc: 0.643 (best 0.643)\n",
            "In epoch 25, loss: 0.934, val acc: 0.672 (best 0.672), test acc: 0.672 (best 0.662)\n",
            "In epoch 30, loss: 0.641, val acc: 0.712 (best 0.712), test acc: 0.691 (best 0.691)\n",
            "In epoch 35, loss: 0.409, val acc: 0.718 (best 0.718), test acc: 0.719 (best 0.719)\n",
            "In epoch 40, loss: 0.250, val acc: 0.724 (best 0.724), test acc: 0.736 (best 0.727)\n",
            "In epoch 45, loss: 0.150, val acc: 0.724 (best 0.730), test acc: 0.742 (best 0.738)\n",
            "In epoch 50, loss: 0.091, val acc: 0.724 (best 0.730), test acc: 0.743 (best 0.738)\n",
            "In epoch 55, loss: 0.058, val acc: 0.726 (best 0.730), test acc: 0.748 (best 0.738)\n",
            "In epoch 60, loss: 0.039, val acc: 0.728 (best 0.730), test acc: 0.748 (best 0.738)\n",
            "In epoch 65, loss: 0.028, val acc: 0.740 (best 0.740), test acc: 0.749 (best 0.749)\n",
            "In epoch 70, loss: 0.021, val acc: 0.746 (best 0.746), test acc: 0.747 (best 0.747)\n",
            "In epoch 75, loss: 0.017, val acc: 0.746 (best 0.746), test acc: 0.752 (best 0.747)\n",
            "In epoch 80, loss: 0.014, val acc: 0.746 (best 0.746), test acc: 0.752 (best 0.747)\n",
            "In epoch 85, loss: 0.012, val acc: 0.746 (best 0.746), test acc: 0.753 (best 0.747)\n",
            "In epoch 90, loss: 0.010, val acc: 0.746 (best 0.746), test acc: 0.756 (best 0.747)\n",
            "In epoch 95, loss: 0.009, val acc: 0.748 (best 0.748), test acc: 0.753 (best 0.755)\n",
            "In epoch 100, loss: 0.008, val acc: 0.746 (best 0.748), test acc: 0.754 (best 0.755)\n",
            "In epoch 105, loss: 0.008, val acc: 0.746 (best 0.748), test acc: 0.754 (best 0.755)\n",
            "In epoch 110, loss: 0.007, val acc: 0.744 (best 0.748), test acc: 0.755 (best 0.755)\n",
            "In epoch 115, loss: 0.006, val acc: 0.744 (best 0.748), test acc: 0.755 (best 0.755)\n",
            "In epoch 120, loss: 0.006, val acc: 0.744 (best 0.748), test acc: 0.756 (best 0.755)\n",
            "In epoch 125, loss: 0.006, val acc: 0.744 (best 0.748), test acc: 0.757 (best 0.755)\n",
            "In epoch 130, loss: 0.005, val acc: 0.744 (best 0.748), test acc: 0.756 (best 0.755)\n",
            "In epoch 135, loss: 0.005, val acc: 0.744 (best 0.748), test acc: 0.755 (best 0.755)\n",
            "In epoch 140, loss: 0.005, val acc: 0.744 (best 0.748), test acc: 0.755 (best 0.755)\n",
            "In epoch 145, loss: 0.004, val acc: 0.744 (best 0.748), test acc: 0.755 (best 0.755)\n",
            "In epoch 150, loss: 0.004, val acc: 0.744 (best 0.748), test acc: 0.757 (best 0.755)\n",
            "In epoch 155, loss: 0.004, val acc: 0.744 (best 0.748), test acc: 0.757 (best 0.755)\n",
            "In epoch 160, loss: 0.004, val acc: 0.744 (best 0.748), test acc: 0.757 (best 0.755)\n",
            "In epoch 165, loss: 0.004, val acc: 0.744 (best 0.748), test acc: 0.758 (best 0.755)\n",
            "In epoch 170, loss: 0.003, val acc: 0.744 (best 0.748), test acc: 0.758 (best 0.755)\n",
            "In epoch 175, loss: 0.003, val acc: 0.744 (best 0.748), test acc: 0.758 (best 0.755)\n",
            "In epoch 180, loss: 0.003, val acc: 0.744 (best 0.748), test acc: 0.758 (best 0.755)\n",
            "In epoch 185, loss: 0.003, val acc: 0.742 (best 0.748), test acc: 0.757 (best 0.755)\n",
            "In epoch 190, loss: 0.003, val acc: 0.742 (best 0.748), test acc: 0.757 (best 0.755)\n",
            "In epoch 195, loss: 0.003, val acc: 0.742 (best 0.748), test acc: 0.757 (best 0.755)\n"
          ]
        }
      ],
      "source": [
        "import dgl.data\n",
        "\n",
        "dataset = dgl.data.CoraGraphDataset()\n",
        "g = dataset[0]\n",
        "\n",
        "def train(g, model):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    all_logits = []\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    features = g.ndata['feat']\n",
        "    labels = g.ndata['label']\n",
        "    train_mask = g.ndata['train_mask']\n",
        "    val_mask = g.ndata['val_mask']\n",
        "    test_mask = g.ndata['test_mask']\n",
        "    for e in range(200):\n",
        "        # Forward\n",
        "        logits = model(g, features)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        # Compute loss\n",
        "        # Note that we should only compute the losses of the nodes in the training set,\n",
        "        # i.e. with train_mask 1.\n",
        "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "\n",
        "        # Compute accuracy on training/validation/test\n",
        "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
        "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
        "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
        "\n",
        "        # Save the best validation accuracy and the corresponding test accuracy.\n",
        "        if best_val_acc < val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        all_logits.append(logits.detach())\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
        "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
        "\n",
        "model = Model(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
        "train(g, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STSQr9jtQ0VG"
      },
      "source": [
        "More customization\n",
        "------------------\n",
        "\n",
        "In DGL, we provide many built-in message and reduce functions under the\n",
        "``dgl.function`` package. You can find more details in `the API\n",
        "doc <apifunction>`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5OuuzZMQ0VG"
      },
      "source": [
        "These APIs allow one to quickly implement new graph convolution modules.\n",
        "For example, the following implements a new ``SAGEConv`` that aggregates\n",
        "neighbor representations using a weighted average. Note that ``edata``\n",
        "member can hold edge features which can also take part in message\n",
        "passing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TbKuvvXcQ0VH"
      },
      "outputs": [],
      "source": [
        "class WeightedSAGEConv(nn.Module):\n",
        "    \"\"\"Graph convolution module used by the GraphSAGE model with edge weights.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_feat : int\n",
        "        Input feature size.\n",
        "    out_feat : int\n",
        "        Output feature size.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_feat, out_feat):\n",
        "        super(WeightedSAGEConv, self).__init__()\n",
        "        # A linear submodule for projecting the input and neighbor feature to the output.\n",
        "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
        "\n",
        "    def forward(self, g, h, w):\n",
        "        \"\"\"Forward computation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        g : Graph\n",
        "            The input graph.\n",
        "        h : Tensor\n",
        "            The input node feature.\n",
        "        w : Tensor\n",
        "            The edge weight.\n",
        "        \"\"\"\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.edata['w'] = w\n",
        "            g.update_all(message_func=fn.u_mul_e('h', 'w', 'm'), reduce_func=fn.mean('m', 'h_N'))\n",
        "            #Builtin message function that computes a message on an edge by performing element-wise mul between features of u and e if the\n",
        "            #features have the same shape; otherwise, it first broadcasts the features to a new shape and performs the element-wise operation\n",
        "            h_N = g.ndata['h_N']\n",
        "            h_total = torch.cat([h, h_N], dim=1)\n",
        "            return self.linear(h_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yno8fpAmQ0VH"
      },
      "source": [
        "Because the graph in this dataset does not have edge weights, we\n",
        "manually assign all edge weights to one in the ``forward()`` function of\n",
        "the model. You can replace it with your own edge weights.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04BJU1J9Q0VH",
        "outputId": "6943a444-d1eb-440d-cc04-001f51785a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 1.951, val acc: 0.122 (best 0.122), test acc: 0.130 (best 0.130)\n",
            "In epoch 5, loss: 1.859, val acc: 0.462 (best 0.468), test acc: 0.472 (best 0.466)\n",
            "In epoch 10, loss: 1.705, val acc: 0.340 (best 0.468), test acc: 0.356 (best 0.466)\n",
            "In epoch 15, loss: 1.484, val acc: 0.402 (best 0.468), test acc: 0.418 (best 0.466)\n",
            "In epoch 20, loss: 1.212, val acc: 0.480 (best 0.480), test acc: 0.481 (best 0.481)\n",
            "In epoch 25, loss: 0.921, val acc: 0.542 (best 0.542), test acc: 0.556 (best 0.556)\n",
            "In epoch 30, loss: 0.650, val acc: 0.584 (best 0.584), test acc: 0.592 (best 0.592)\n",
            "In epoch 35, loss: 0.425, val acc: 0.638 (best 0.638), test acc: 0.634 (best 0.634)\n",
            "In epoch 40, loss: 0.261, val acc: 0.696 (best 0.696), test acc: 0.681 (best 0.681)\n",
            "In epoch 45, loss: 0.156, val acc: 0.722 (best 0.722), test acc: 0.701 (best 0.701)\n",
            "In epoch 50, loss: 0.095, val acc: 0.732 (best 0.732), test acc: 0.714 (best 0.715)\n",
            "In epoch 55, loss: 0.060, val acc: 0.730 (best 0.732), test acc: 0.713 (best 0.715)\n",
            "In epoch 60, loss: 0.041, val acc: 0.736 (best 0.736), test acc: 0.705 (best 0.711)\n",
            "In epoch 65, loss: 0.029, val acc: 0.732 (best 0.736), test acc: 0.708 (best 0.711)\n",
            "In epoch 70, loss: 0.023, val acc: 0.732 (best 0.736), test acc: 0.710 (best 0.711)\n",
            "In epoch 75, loss: 0.018, val acc: 0.732 (best 0.736), test acc: 0.710 (best 0.711)\n",
            "In epoch 80, loss: 0.015, val acc: 0.732 (best 0.736), test acc: 0.709 (best 0.711)\n",
            "In epoch 85, loss: 0.013, val acc: 0.734 (best 0.736), test acc: 0.709 (best 0.711)\n",
            "In epoch 90, loss: 0.011, val acc: 0.734 (best 0.736), test acc: 0.708 (best 0.711)\n",
            "In epoch 95, loss: 0.010, val acc: 0.734 (best 0.736), test acc: 0.708 (best 0.711)\n",
            "In epoch 100, loss: 0.009, val acc: 0.734 (best 0.736), test acc: 0.707 (best 0.711)\n",
            "In epoch 105, loss: 0.008, val acc: 0.734 (best 0.736), test acc: 0.709 (best 0.711)\n",
            "In epoch 110, loss: 0.008, val acc: 0.732 (best 0.736), test acc: 0.709 (best 0.711)\n",
            "In epoch 115, loss: 0.007, val acc: 0.732 (best 0.736), test acc: 0.710 (best 0.711)\n",
            "In epoch 120, loss: 0.007, val acc: 0.734 (best 0.736), test acc: 0.712 (best 0.711)\n",
            "In epoch 125, loss: 0.006, val acc: 0.734 (best 0.736), test acc: 0.712 (best 0.711)\n",
            "In epoch 130, loss: 0.006, val acc: 0.734 (best 0.736), test acc: 0.712 (best 0.711)\n",
            "In epoch 135, loss: 0.005, val acc: 0.734 (best 0.736), test acc: 0.712 (best 0.711)\n",
            "In epoch 140, loss: 0.005, val acc: 0.734 (best 0.736), test acc: 0.711 (best 0.711)\n",
            "In epoch 145, loss: 0.005, val acc: 0.736 (best 0.736), test acc: 0.711 (best 0.711)\n",
            "In epoch 150, loss: 0.005, val acc: 0.736 (best 0.736), test acc: 0.713 (best 0.711)\n",
            "In epoch 155, loss: 0.004, val acc: 0.736 (best 0.736), test acc: 0.713 (best 0.711)\n",
            "In epoch 160, loss: 0.004, val acc: 0.736 (best 0.736), test acc: 0.713 (best 0.711)\n",
            "In epoch 165, loss: 0.004, val acc: 0.736 (best 0.736), test acc: 0.713 (best 0.711)\n",
            "In epoch 170, loss: 0.004, val acc: 0.738 (best 0.738), test acc: 0.713 (best 0.713)\n",
            "In epoch 175, loss: 0.004, val acc: 0.738 (best 0.738), test acc: 0.713 (best 0.713)\n",
            "In epoch 180, loss: 0.003, val acc: 0.740 (best 0.740), test acc: 0.713 (best 0.713)\n",
            "In epoch 185, loss: 0.003, val acc: 0.740 (best 0.740), test acc: 0.713 (best 0.713)\n",
            "In epoch 190, loss: 0.003, val acc: 0.740 (best 0.740), test acc: 0.713 (best 0.713)\n",
            "In epoch 195, loss: 0.003, val acc: 0.740 (best 0.740), test acc: 0.713 (best 0.713)\n"
          ]
        }
      ],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = WeightedSAGEConv(in_feats, h_feats)\n",
        "        self.conv2 = WeightedSAGEConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat, torch.ones(g.num_edges(), 1).to(g.device))\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h, torch.ones(g.num_edges(), 1).to(g.device))\n",
        "        return h\n",
        "\n",
        "model = Model(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
        "train(g, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZbK57RBQ0VH"
      },
      "source": [
        "Even more customization by user-defined function\n",
        "------------------------------------------------\n",
        "\n",
        "DGL allows user-defined message and reduce function for the maximal\n",
        "expressiveness. Here is a user-defined message function that is\n",
        "equivalent to ``fn.u_mul_e('h', 'w', 'm')``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dPDl0xkQQ0VI"
      },
      "outputs": [],
      "source": [
        "def u_mul_e_udf(edges):\n",
        "    return {'m' : edges.src['h'] * edges.data['w']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5syB5lLQ0VI"
      },
      "source": [
        "``edges`` has three members: ``src``, ``data`` and ``dst``, representing\n",
        "the source node feature, edge feature, and destination node feature for\n",
        "all edges.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Hi0NzpQ0VI"
      },
      "source": [
        "You can also write your own reduce function. For example, the following\n",
        "is equivalent to the builtin ``fn.mean('m', 'h_N')`` function that averages\n",
        "the incoming messages:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LgiwwW88Q0VI"
      },
      "outputs": [],
      "source": [
        "def mean_udf(nodes):\n",
        "    return {'h_N': nodes.mailbox['m'].mean(1)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2NtJ0gYQ0VI"
      },
      "source": [
        "In short, DGL will group the nodes by their in-degrees, and for each\n",
        "group DGL stacks the incoming messages along the second dimension. You\n",
        "can then perform a reduction along the second dimension to aggregate\n",
        "messages.\n",
        "\n",
        "For more details on customizing message and reduce function with\n",
        "user-defined function, please refer to the `API\n",
        "reference <apiudf>`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdA67dkVQ0VJ"
      },
      "source": [
        "Best practice of writing custom GNN modules\n",
        "-------------------------------------------\n",
        "\n",
        "DGL recommends the following practice ranked by preference:\n",
        "\n",
        "-  Use ``dgl.nn`` modules.\n",
        "-  Use ``dgl.nn.functional`` functions which contain lower-level complex\n",
        "   operations such as computing a softmax for each node over incoming\n",
        "   edges.\n",
        "-  Use ``update_all`` with builtin message and reduce functions.\n",
        "-  Use user-defined message or reduce functions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP04zZw0Q0VJ"
      },
      "source": [
        "What’s next?\n",
        "------------\n",
        "\n",
        "-  `Writing Efficient Message Passing\n",
        "   Code <guide-message-passing-efficient>`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xxnhNZOsQ0VJ"
      },
      "outputs": [],
      "source": [
        "# Thumbnail credits: Representation Learning on Networks, Jure Leskovec, WWW 2018\n",
        "# sphinx_gallery_thumbnail_path = '_static/blitz_3_message_passing.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}